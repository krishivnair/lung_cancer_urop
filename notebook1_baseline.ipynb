{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**‚ÄúExplainable Lung Cancer Classification via Hypergraph Neural Networks Modeling Inter-Nodule Relationships‚Äù**"
      ],
      "metadata": {
        "id": "YZ4HS0OUg41R"
      },
      "id": "YZ4HS0OUg41R"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "CELL 1: MOUNT DRIVE & SETUP\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/lung_cancer_urop')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚úì Drive mounted and path added\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "u9k-lskpfCxP"
      },
      "id": "u9k-lskpfCxP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "CELL 2: INSTALL DEPENDENCIES\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "!pip install -q nibabel SimpleITK torch-geometric wandb\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚úì Dependencies installed\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "UU3hcVBsfFTW"
      },
      "id": "UU3hcVBsfFTW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "CELL 3: IMPORT MODULES\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from src.config import ExperimentConfig\n",
        "from src.utils import set_global_seed, setup_directories\n",
        "from src.preprocessing import AdvancedPreprocessor\n",
        "from src.hypergraph import HypergraphConstructor\n",
        "from src.models import HypergraphNeuralNetwork\n",
        "from src.dataset import LungNoduleHypergraphDataset, collate_hypergraph_batch\n",
        "from src.early_stopping import EarlyStopping\n",
        "from src.trainer import HGNNTrainer\n",
        "from src.visualization import ResultsVisualizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚úì ALL MODULES IMPORTED SUCCESSFULLY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "YCUsIDO7fMqg"
      },
      "id": "YCUsIDO7fMqg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "CELL 4: INITIALIZE CONFIGURATION\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "config = ExperimentConfig()\n",
        "set_global_seed(config.random_seed)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONFIGURATION INITIALIZED\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Experiment: {config.experiment_name}\")\n",
        "print(f\"Random Seed: {config.random_seed}\")\n",
        "print(f\"Num Patients: {config.num_patients}\")\n",
        "print(f\"Batch Size: {config.batch_size}\")\n",
        "print(f\"Learning Rate: {config.learning_rate}\")\n",
        "print(f\"Num Epochs: {config.num_epochs}\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "nZ1Jj_GrfQO3"
      },
      "id": "nZ1Jj_GrfQO3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "CELL 5: SETUP DIRECTORIES & LOAD DATA\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "BASE_PATH = Path(config.base_path)\n",
        "SUBSET_PATH = BASE_PATH / config.subset_name\n",
        "ANNOTATIONS_PATH = BASE_PATH / config.annotations_file\n",
        "METADATA_PATH = BASE_PATH / config.metadata_file\n",
        "\n",
        "OUTPUT_PATH, MODELS_PATH, RESULTS_PATH, LOGS_PATH, CONFIG_PATH = setup_directories(\n",
        "    BASE_PATH, config.experiment_name\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DIRECTORY STRUCTURE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"‚úì Base Path: {BASE_PATH}\")\n",
        "print(f\"‚úì Output Path: {OUTPUT_PATH}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Verify\n",
        "assert SUBSET_PATH.exists(), f\"Subset path not found: {SUBSET_PATH}\"\n",
        "assert ANNOTATIONS_PATH.exists(), f\"Annotations not found: {ANNOTATIONS_PATH}\"\n",
        "assert METADATA_PATH.exists(), f\"Metadata not found: {METADATA_PATH}\"\n",
        "\n",
        "# Load CSVs\n",
        "annotations_df = pd.read_csv(ANNOTATIONS_PATH)\n",
        "metadata_df = pd.read_csv(METADATA_PATH)\n",
        "\n",
        "print(f\"\\n‚úì Annotations: {annotations_df.shape}\")\n",
        "print(f\"‚úì Metadata: {metadata_df.shape}\")\n",
        "\n",
        "# Get patient files\n",
        "# Get patient files from multiple subsets\n",
        "patient_files = []\n",
        "subsets_to_load = [\"subset01\", \"subset02\", \"subset03\"]  # Add \"subset04\" later\n",
        "\n",
        "print(\"\\nLOADING SUBSETS:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for subset_name in subsets_to_load:\n",
        "    subset_path = BASE_PATH / subset_name\n",
        "    if subset_path.exists():\n",
        "        files = sorted(list(subset_path.glob(\"*.nii.gz\")))\n",
        "        patient_files.extend(files)\n",
        "\n",
        "        # Determine patient range\n",
        "        if subset_name == \"subset01\":\n",
        "            patient_range = \"1-160\"\n",
        "        elif subset_name == \"subset02\":\n",
        "            patient_range = \"161-320\"\n",
        "        elif subset_name == \"subset03\":\n",
        "            patient_range = \"321-480\"\n",
        "        else:\n",
        "            patient_range = \"unknown\"\n",
        "\n",
        "        print(f\"‚úì {subset_name}: {len(files)} files (Patients {patient_range})\")\n",
        "    else:\n",
        "        print(f\"‚ùå {subset_name}: NOT FOUND at {subset_path}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(f\"‚úì TOTAL LOADED: {len(patient_files)} patient files\")\n",
        "\n",
        "# Apply num_patients limit if needed (optional)\n",
        "if config.num_patients < len(patient_files):\n",
        "    patient_files = patient_files[:config.num_patients]\n",
        "    print(f\"‚ö†Ô∏è Limited to first {config.num_patients} patients (config setting)\")\n",
        "\n",
        "# Save config\n",
        "config.save_config(str(CONFIG_PATH / \"experiment_config.yaml\"))\n",
        "print(f\"‚úì Config saved\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "eioXSblEfdtE"
      },
      "id": "eioXSblEfdtE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "CELL 6: INITIALIZE EXPERIMENT TRACKING\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "# Use local Colab storage instead of Drive for TensorBoard\n",
        "tensorboard_local_dir = Path('/content/logs/tensorboard')\n",
        "tensorboard_local_dir.mkdir(parents=True, exist_ok=True)\n",
        "tensorboard_writer = SummaryWriter(log_dir=str(tensorboard_local_dir))\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EXPERIMENT TRACKING\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"‚úì TensorBoard: {tensorboard_local_dir} (local - avoiding Drive disconnects)\")\n",
        "\n",
        "try:\n",
        "    import wandb\n",
        "    wandb.login()\n",
        "    wandb.init(\n",
        "        project=config.project_name,\n",
        "        name=config.experiment_name,\n",
        "        config=vars(config),\n",
        "        tags=[\"hgnn\", \"lung-cancer\", \"baseline\"]\n",
        "    )\n",
        "    print(f\"‚úì W&B: {wandb.run.url}\")\n",
        "    USING_WANDB = True\n",
        "except:\n",
        "    print(\"‚ö† W&B not available - using TensorBoard only\")\n",
        "    USING_WANDB = False\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "WrgEJf5KfhQ2"
      },
      "id": "WrgEJf5KfhQ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "CELL 7: INITIALIZE PREPROCESSING & HYPERGRAPH\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "preprocessor = AdvancedPreprocessor(\n",
        "    target_spacing=config.target_spacing,\n",
        "    target_size=config.patch_size\n",
        ")\n",
        "\n",
        "hypergraph_constructor = HypergraphConstructor(\n",
        "    k_neighbors=config.k_neighbors,\n",
        "    spatial_threshold=config.spatial_threshold,\n",
        "    feature_threshold=config.feature_similarity_threshold\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PREPROCESSING & HYPERGRAPH INITIALIZED\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"‚úì Target spacing: {config.target_spacing}\")\n",
        "print(f\"‚úì Patch size: {config.patch_size}\")\n",
        "print(f\"‚úì k-neighbors: {config.k_neighbors}\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "F_bvJyk7fk1B"
      },
      "id": "F_bvJyk7fk1B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "=======================================================\n",
        "CELL 7.5: VISUALIZE PREPROCESSING\n",
        "=======================================================\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "\n",
        "subset01 = Path(\"/content/drive/MyDrive/duke_lung_data/subset01\")\n",
        "subset02 = Path(\"/content/drive/MyDrive/duke_lung_data/subset02\")\n",
        "subset03 = Path(\"/content/drive/MyDrive/duke_lung_data/subset3\")\n",
        "\n",
        "print(\"Total patient files:\", len(patient_files))\n",
        "assert all(p.exists() for p in patient_files)\n"
      ],
      "metadata": {
        "id": "H230WKhQJS5p"
      },
      "id": "H230WKhQJS5p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "CELL 8: ROBUST DATASET & DATALOADERS (COLAB-FRIENDLY)\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Batch\n",
        "import torch\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CREATING ROBUST DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define collate function inline (no import needed)\n",
        "def collate_hypergraph_batch(batch):\n",
        "    \"\"\"Filter out None values and batch remaining data.\"\"\"\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "    return Batch.from_data_list(batch)\n",
        "\n",
        "# Create dataset\n",
        "dataset = LungNoduleHypergraphDataset(\n",
        "    patient_files=patient_files,\n",
        "    annotations_df=annotations_df,\n",
        "    preprocessor=preprocessor,\n",
        "    hypergraph_constructor=hypergraph_constructor,\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "# Filter valid samples\n",
        "print(\"\\nüîç Filtering valid samples...\")\n",
        "valid_indices = []\n",
        "error_count = 0\n",
        "\n",
        "for idx in range(len(dataset)):\n",
        "    try:\n",
        "        data = dataset[idx]\n",
        "        if data is not None:\n",
        "            valid_indices.append(idx)\n",
        "        else:\n",
        "            error_count += 1\n",
        "    except Exception as e:\n",
        "        error_count += 1\n",
        "        if error_count <= 5:  # Print first 5 errors only\n",
        "            print(f\"‚ö†Ô∏è Sample {idx} failed: {str(e)[:100]}\")\n",
        "\n",
        "print(f\"‚úì Valid samples: {len(valid_indices)} / {len(dataset)}\")\n",
        "print(f\"‚úó Invalid/missing samples: {error_count}\")\n",
        "\n",
        "if len(valid_indices) == 0:\n",
        "    raise RuntimeError(\"‚ùå No valid samples found! Check your data paths and files.\")\n",
        "\n",
        "# Filtered dataset wrapper\n",
        "class FilteredDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_dataset, valid_indices):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.valid_indices = valid_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.base_dataset[self.valid_indices[idx]]\n",
        "\n",
        "filtered_dataset = FilteredDataset(dataset, valid_indices)\n",
        "\n",
        "# Split\n",
        "train_size = int(config.train_split * len(filtered_dataset))\n",
        "val_size = len(filtered_dataset) - train_size\n",
        "\n",
        "generator = torch.Generator().manual_seed(config.random_seed)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    filtered_dataset, [train_size, val_size], generator=generator\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Train samples: {train_size}\")\n",
        "print(f\"‚úì Val samples: {val_size}\")\n",
        "\n",
        "# DataLoaders with custom collate\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=False,\n",
        "    collate_fn=collate_hypergraph_batch\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=False,\n",
        "    collate_fn=collate_hypergraph_batch\n",
        ")\n",
        "\n",
        "print(f\"‚úì Train batches: {len(train_loader)}\")\n",
        "print(f\"‚úì Val batches: {len(val_loader)}\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "1b6bwkDLfzv-",
        "collapsed": true
      },
      "id": "1b6bwkDLfzv-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "CELL 8.5: FORCE CLASS BALANCE (OVERSAMPLING) - ROBUST VERSION\n",
        "Run this RIGHT AFTER loading your dataset (Cell 8) and BEFORE training (Cell 11).\n",
        "================================================================================\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from collections import Counter\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üöë APPLYING ROBUST OVERSAMPLING (Wrapper Method)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Extract Labels safely\n",
        "# We iterate over the train_dataset as-is, treating it as a black box.\n",
        "num_samples = len(train_dataset)\n",
        "all_indices = list(range(num_samples))\n",
        "train_labels = []\n",
        "\n",
        "print(f\"Scanning {num_samples} patients for labels...\")\n",
        "\n",
        "for i in all_indices:\n",
        "    try:\n",
        "        # Get the data object (graph)\n",
        "        data = train_dataset[i]\n",
        "\n",
        "        # Extract label safely (handle both 0-d and 1-d tensors)\n",
        "        if hasattr(data, 'y'):\n",
        "            label_val = data.y.item() if data.y.numel() == 1 else data.y[0].item()\n",
        "            train_labels.append(label_val)\n",
        "        else:\n",
        "            # Fallback for weird edge cases\n",
        "            train_labels.append(0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Warning: Could not read label for index {i}: {e}\")\n",
        "        train_labels.append(0) # Assume Benign if broken\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# 2. Check Distribution\n",
        "unique, counts = np.unique(train_labels, return_counts=True)\n",
        "print(f\"\\nOriginal Distribution:\")\n",
        "for cls, c in zip(unique, counts):\n",
        "    print(f\"  Class {cls}: {c} patients\")\n",
        "\n",
        "# 3. Calculate INDICES to repeat\n",
        "# We want to match the majority class count\n",
        "class_counts = Counter(train_labels)\n",
        "if len(class_counts) > 0:\n",
        "    max_count = max(class_counts.values())\n",
        "else:\n",
        "    max_count = 0\n",
        "    print(\"‚ùå CRITICAL: No labels found.\")\n",
        "\n",
        "final_indices = []\n",
        "\n",
        "print(f\"\\nTargeting {max_count} samples per class...\")\n",
        "\n",
        "for cls in unique:\n",
        "    # Get indices for this class\n",
        "    cls_indices = [i for i, label in enumerate(train_labels) if label == cls]\n",
        "\n",
        "    if len(cls_indices) == 0: continue\n",
        "\n",
        "    # Math to fill the gap\n",
        "    n_current = len(cls_indices)\n",
        "    n_repeat = max_count // n_current\n",
        "    n_remainder = max_count % n_current\n",
        "\n",
        "    # Add full repeats\n",
        "    final_indices.extend(cls_indices * n_repeat)\n",
        "    # Add random remainder to hit exact target\n",
        "    final_indices.extend(cls_indices[:n_remainder])\n",
        "\n",
        "    print(f\"  Class {cls}: Oversampled from {n_current} -> {len(cls_indices * n_repeat) + n_remainder}\")\n",
        "\n",
        "# 4. Create Safe Balanced Loader\n",
        "if len(final_indices) > 0:\n",
        "    # üö® THE FIX: Wrap 'train_dataset' DIRECTLY. Do not use .dataset\n",
        "    # This creates a \"Subset of a Subset\", which preserves all previous filters/splits.\n",
        "    balanced_train_dataset = Subset(train_dataset, final_indices)\n",
        "\n",
        "    # Overwrite the loader\n",
        "    train_loader = DataLoader(\n",
        "        balanced_train_dataset,\n",
        "        batch_size=train_loader.batch_size,\n",
        "        shuffle=True,                       # Shuffle is mandatory here\n",
        "        num_workers=0,\n",
        "        collate_fn=collate_hypergraph_batch\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚úÖ SUCCESS: Balanced Train Loader Ready.\")\n",
        "    print(f\"  Total Training Samples: {len(balanced_train_dataset)}\")\n",
        "\n",
        "    # 5. Sanity Check (Test one batch)\n",
        "    try:\n",
        "        test_batch = next(iter(train_loader))\n",
        "        print(f\"  ‚úì Sanity Check Passed: Loaded a batch of {test_batch.num_graphs} graphs.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Sanity Check Failed: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ùå FAILED: No indices generated.\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "39CTa5fQATL4"
      },
      "id": "39CTa5fQATL4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "CELL 9: INITIALIZE MODEL\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "# Get feature dimension from sample\n",
        "sample_data = None\n",
        "for i in range(len(train_dataset)):\n",
        "    data = train_dataset.dataset[train_dataset.indices[i]]\n",
        "    if data is not None:\n",
        "        sample_data = data\n",
        "        break\n",
        "\n",
        "assert sample_data is not None, \"No valid samples found!\"\n",
        "\n",
        "in_channels = sample_data.x.shape[1]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL INITIALIZATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"‚úì Input features: {in_channels}\")\n",
        "\n",
        "device = torch.device(config.device if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"‚úì Device: {device}\")\n",
        "\n",
        "model = HypergraphNeuralNetwork(\n",
        "    in_channels=in_channels,\n",
        "    hidden_channels=config.hidden_channels,\n",
        "    num_classes=config.num_classes,\n",
        "    num_layers=config.num_layers,\n",
        "    dropout=config.dropout\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"‚úì Total parameters: {total_params:,}\")\n",
        "print(f\"‚úì Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "if USING_WANDB:\n",
        "    wandb.config.update({\n",
        "        'total_parameters': total_params,\n",
        "        'trainable_parameters': trainable_params,\n",
        "        'input_features': in_channels\n",
        "    })\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "JjjR6AW6f2_P"
      },
      "id": "JjjR6AW6f2_P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "CELL 10: TRAINING SETUP (WITH MODULE RELOAD)\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "# Reload config to pick up changes\n",
        "import importlib\n",
        "import src.config\n",
        "importlib.reload(src.config)\n",
        "from src.config import ExperimentConfig\n",
        "\n",
        "# Recreate config with new attributes\n",
        "config = ExperimentConfig()\n",
        "\n",
        "# Now initialize trainer\n",
        "trainer = HGNNTrainer(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    output_dir=MODELS_PATH,\n",
        "    patience=config.patience,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "trainer.setup_training(\n",
        "    lr=config.learning_rate,\n",
        "    weight_decay=config.weight_decay\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TRAINER INITIALIZED\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"‚úì Optimizer: AdamW\")\n",
        "print(f\"‚úì Learning rate: {config.learning_rate}\")\n",
        "print(f\"‚úì LR factor: {config.lr_factor}\")\n",
        "print(f\"‚úì LR patience: {config.lr_patience}\")\n",
        "print(f\"‚úì Mixed precision: {config.use_mixed_precision}\")\n",
        "print(f\"‚úì Early stopping patience: {config.patience}\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "Rg54TJwGgRc4"
      },
      "id": "Rg54TJwGgRc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "FINAL TRAINING CELL: PROFESSIONAL SAMPLER + SAFE WEIGHTS  11\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import copy\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üß™ PROFESSIONAL RUN: WEIGHTED SAMPLER + SAFE WEIGHTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. EXTRACT TRAINING LABELS FOR SAMPLER\n",
        "# ==============================================================================\n",
        "print(\"\\n‚öñÔ∏è Configuring Weighted Sampler...\")\n",
        "\n",
        "train_labels = []\n",
        "valid_train_indices = []\n",
        "\n",
        "for i in range(len(train_dataset)):\n",
        "    data = train_dataset[i]\n",
        "    if data is not None:\n",
        "        train_labels.append(data.y.item())\n",
        "        valid_train_indices.append(i)\n",
        "\n",
        "if len(train_labels) == 0:\n",
        "    raise ValueError(\"‚ùå No valid training samples found!\")\n",
        "\n",
        "class_counts = Counter(train_labels)\n",
        "n_benign = class_counts[0]\n",
        "n_cancer = class_counts[1]\n",
        "\n",
        "print(f\"Training Distribution:\")\n",
        "print(f\"  - Benign: {n_benign}\")\n",
        "print(f\"  - Cancer: {n_cancer}\")\n",
        "print(f\"  - Ratio: {n_benign/n_cancer:.2f}:1\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CREATE WEIGHTED SAMPLER\n",
        "# ==============================================================================\n",
        "# Calculate inverse frequency weights\n",
        "weight_benign = 1.0 / n_benign\n",
        "weight_cancer = 1.0 / n_cancer\n",
        "\n",
        "# Assign weights to each sample\n",
        "samples_weight = torch.tensor(\n",
        "    [weight_cancer if label == 1 else weight_benign for label in train_labels]\n",
        ")\n",
        "\n",
        "# Create sampler\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=samples_weight.type(torch.DoubleTensor),\n",
        "    num_samples=len(samples_weight),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Sampler configured for {len(train_labels)} samples\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. CREATE DATA LOADERS\n",
        "# ==============================================================================\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    sampler=sampler,  # ‚Üê This replaces shuffle=True\n",
        "    collate_fn=collate_hypergraph_batch,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_hypergraph_batch,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Loaders created\")\n",
        "print(f\"  - Train batches: {len(train_loader)}\")\n",
        "print(f\"  - Val batches: {len(val_loader)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. INITIALIZE MODEL\n",
        "# ==============================================================================\n",
        "print(\"\\nüèóÔ∏è Initializing Model...\")\n",
        "\n",
        "# Get feature dimensions from first batch\n",
        "try:\n",
        "    sample = next(iter(train_loader))\n",
        "    in_feats = sample.x.shape[1]\n",
        "    print(f\"Input features: {in_feats}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not detect features, using default 128\")\n",
        "    in_feats = 128\n",
        "\n",
        "model = HypergraphNeuralNetwork(\n",
        "    in_channels=in_feats,\n",
        "    hidden_channels=256,\n",
        "    num_classes=2,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "print(f\"‚úÖ Model initialized\")\n",
        "print(f\"  - Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. SETUP LOSS & OPTIMIZER (SAFE WEIGHTS)\n",
        "# ==============================================================================\n",
        "# ‚ö†Ô∏è CRITICAL: Sampler balances data, so we use GENTLE loss weight\n",
        "# Weight 1.1 = slight nudge, not panic\n",
        "weights = torch.tensor([1.0, 1.1]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-2\n",
        ")\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='max',\n",
        "    factor=0.5,\n",
        "    patience=3\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Training Setup:\")\n",
        "print(f\"  - Loss weights: Benign=1.0, Cancer=1.5\")\n",
        "print(f\"  - Learning rate: 1e-5\")\n",
        "print(f\"  - Optimizer: AdamW\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. TRAINING LOOP\n",
        "# ==============================================================================\n",
        "num_epochs = 50\n",
        "best_f1 = 0.0\n",
        "best_model_state = None\n",
        "\n",
        "print(f\"\\nüöÄ Training for {num_epochs} epochs...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # ============= TRAINING =============\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        if batch is None:\n",
        "            continue\n",
        "\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        try:\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "        except:\n",
        "            out = model(batch.x, batch.edge_index)\n",
        "\n",
        "        if isinstance(out, tuple):\n",
        "            out = out[0]\n",
        "\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # ============= VALIDATION =============\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            try:\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            except:\n",
        "                out = model(batch.x, batch.edge_index)\n",
        "\n",
        "            if isinstance(out, tuple):\n",
        "                out = out[0]\n",
        "\n",
        "            preds = out.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    # ============= METRICS =============\n",
        "    if len(all_preds) > 0:\n",
        "        report = classification_report(\n",
        "            all_labels,\n",
        "            all_preds,\n",
        "            output_dict=True,\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        cancer_f1 = report['1']['f1-score']\n",
        "        cancer_recall = report['1']['recall']\n",
        "        cancer_precision = report['1']['precision']\n",
        "        accuracy = report['accuracy']\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step(cancer_f1)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch:02d}/{num_epochs} | \"\n",
        "              f\"Loss: {avg_train_loss:.4f} | \"\n",
        "              f\"Acc: {accuracy:.3f} | \"\n",
        "              f\"Recall: {cancer_recall:.3f} | \"\n",
        "              f\"Prec: {cancer_precision:.3f} | \"\n",
        "              f\"F1: {cancer_f1:.3f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if cancer_f1 >= best_f1:\n",
        "            best_f1 = cancer_f1\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            print(f\"  ‚úì Best F1: {best_f1:.3f}\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚úÖ Training Complete!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. LOAD BEST MODEL & FINAL EVALUATION\n",
        "# ==============================================================================\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    # Save model\n",
        "    save_path = str(MODELS_PATH / \"best_model_professional.pth\")\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"\\n‚úÖ Best model saved: {save_path}\")\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üìä FINAL EVALUATION ON VALIDATION SET\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    model.eval()\n",
        "    final_preds = []\n",
        "    final_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            try:\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            except:\n",
        "                out = model(batch.x, batch.edge_index)\n",
        "\n",
        "            if isinstance(out, tuple):\n",
        "                out = out[0]\n",
        "\n",
        "            final_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "            final_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        final_labels,\n",
        "        final_preds,\n",
        "        target_names=['Benign', 'Cancer'],\n",
        "        zero_division=0\n",
        "    ))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(final_labels, final_preds)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Greens',\n",
        "        cbar=False,\n",
        "        xticklabels=['Benign', 'Cancer'],\n",
        "        yticklabels=['Benign', 'Cancer'],\n",
        "        annot_kws={'size': 16}\n",
        "    )\n",
        "    plt.xlabel('Predicted', fontsize=12)\n",
        "    plt.ylabel('Actual', fontsize=12)\n",
        "    plt.title('Professional Model - Confusion Matrix', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üéâ PROFESSIONAL TRAINING COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No model was saved (no improvement detected)\")\n",
        "\n",
        "print(\"\\nüí° Next Step: Apply threshold tuning (0.46) for even better recall!\")"
      ],
      "metadata": {
        "id": "UCxxEbhagYAu",
        "collapsed": true
      },
      "id": "UCxxEbhagYAu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FEATURE QUALITY DIAGNOSTIC\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"üî¨ ANALYZING FEATURE QUALITY...\")\n",
        "\n",
        "# Collect features from validation set\n",
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "for i in range(len(val_dataset)):\n",
        "    data = val_dataset[i]\n",
        "    if data is not None:\n",
        "        all_features.append(data.x.cpu().numpy())\n",
        "        all_labels.extend([data.y.item()] * len(data.x))\n",
        "\n",
        "# Flatten to (n_samples, n_features)\n",
        "features_flat = np.vstack(all_features)\n",
        "labels_flat = np.array(all_labels)\n",
        "\n",
        "print(f\"Total nodules: {len(labels_flat)}\")\n",
        "print(f\"  Benign: {sum(labels_flat == 0)}\")\n",
        "print(f\"  Cancer: {sum(labels_flat == 1)}\")\n",
        "\n",
        "# Check for zero-variance features\n",
        "feature_stds = features_flat.std(axis=0)\n",
        "zero_var = sum(feature_stds < 1e-6)\n",
        "print(f\"\\n‚ö†Ô∏è Zero-variance features: {zero_var}/{features_flat.shape[1]}\")\n",
        "\n",
        "# Check for NaN/Inf\n",
        "nan_count = np.isnan(features_flat).sum()\n",
        "inf_count = np.isinf(features_flat).sum()\n",
        "print(f\"‚ö†Ô∏è NaN values: {nan_count}\")\n",
        "print(f\"‚ö†Ô∏è Inf values: {inf_count}\")\n",
        "\n",
        "# PCA visualization\n",
        "pca = PCA(n_components=2)\n",
        "features_2d = pca.fit_transform(features_flat)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(features_2d[labels_flat==0, 0], features_2d[labels_flat==0, 1],\n",
        "            alpha=0.5, label='Benign', s=30, c='blue')\n",
        "plt.scatter(features_2d[labels_flat==1, 0], features_2d[labels_flat==1, 1],\n",
        "            alpha=0.7, label='Cancer', s=50, c='red', marker='^')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('Feature Space Visualization (PCA)')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä PCA explained variance: {pca.explained_variance_ratio_.sum():.2%}\")"
      ],
      "metadata": {
        "id": "3k4Eo6VHPgrK"
      },
      "id": "3k4Eo6VHPgrK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#diagnosti\n",
        "\"\"\"\n",
        "DEBUG NODULE EXTRACTION\n",
        "\"\"\"\n",
        "import SimpleITK as sitk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get a cancer case\n",
        "cancer_row = annotations_df[annotations_df['Malignant_lbl'] == 1].iloc[0]\n",
        "patient_id = str(cancer_row['patient-id']).strip()\n",
        "\n",
        "print(f\"Testing Cancer Patient: {patient_id}\")\n",
        "print(f\"World coords: X={cancer_row['coordX']}, Y={cancer_row['coordY']}, Z={cancer_row['coordZ']}\")\n",
        "print(f\"Diameter: {cancer_row['w']}mm\")\n",
        "\n",
        "# Find the file\n",
        "patient_file = None\n",
        "for f in patient_files:\n",
        "    file_id = f.stem.replace('.nii', '')\n",
        "    if file_id == patient_id:\n",
        "        patient_file = f\n",
        "        break\n",
        "\n",
        "if patient_file is None:\n",
        "    print(f\"‚ùå File not found for {patient_id}\")\n",
        "else:\n",
        "    print(f\"‚úì Found file: {patient_file.name}\")\n",
        "\n",
        "    # Load image\n",
        "    image_sitk = sitk.ReadImage(str(patient_file))\n",
        "    image_array = sitk.GetArrayFromImage(image_sitk)\n",
        "\n",
        "    print(f\"\\nImage shape: {image_array.shape} (Z, Y, X)\")\n",
        "\n",
        "    # Try coordinate transform\n",
        "    try:\n",
        "        point_world = (float(cancer_row['coordX']),\n",
        "                       float(cancer_row['coordY']),\n",
        "                       float(cancer_row['coordZ']))\n",
        "\n",
        "        idx_voxel = image_sitk.TransformPhysicalPointToIndex(point_world)\n",
        "        x_voxel, y_voxel, z_voxel = idx_voxel\n",
        "\n",
        "        print(f\"\\nTransformed to voxel coords:\")\n",
        "        print(f\"  X: {x_voxel} (max: {image_array.shape[2]-1})\")\n",
        "        print(f\"  Y: {y_voxel} (max: {image_array.shape[1]-1})\")\n",
        "        print(f\"  Z: {z_voxel} (max: {image_array.shape[0]-1})\")\n",
        "\n",
        "        # Check if within bounds\n",
        "        if (0 <= x_voxel < image_array.shape[2] and\n",
        "            0 <= y_voxel < image_array.shape[1] and\n",
        "            0 <= z_voxel < image_array.shape[0]):\n",
        "\n",
        "            print(\"‚úì Coordinates within bounds\")\n",
        "\n",
        "            # Extract 32x32x32 patch\n",
        "            patch_size = 16\n",
        "            z_start = max(0, z_voxel - patch_size)\n",
        "            z_end = min(image_array.shape[0], z_voxel + patch_size)\n",
        "            y_start = max(0, y_voxel - patch_size)\n",
        "            y_end = min(image_array.shape[1], y_voxel + patch_size)\n",
        "            x_start = max(0, x_voxel - patch_size)\n",
        "            x_end = min(image_array.shape[2], x_voxel + patch_size)\n",
        "\n",
        "            patch = image_array[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "            print(f\"\\nExtracted patch shape: {patch.shape}\")\n",
        "            print(f\"Patch HU range: [{patch.min():.1f}, {patch.max():.1f}]\")\n",
        "            print(f\"Patch mean: {patch.mean():.1f}\")\n",
        "            print(f\"Patch std: {patch.std():.1f}\")\n",
        "\n",
        "            # Visualize center slice\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "            # Axial slice (XY plane at nodule center)\n",
        "            axes[0].imshow(image_array[z_voxel, :, :], cmap='gray', vmin=-1000, vmax=400)\n",
        "            axes[0].plot(x_voxel, y_voxel, 'r+', markersize=15, markeredgewidth=2)\n",
        "            axes[0].set_title(f'Axial Slice (Z={z_voxel})')\n",
        "            axes[0].set_xlabel('X')\n",
        "            axes[0].set_ylabel('Y')\n",
        "\n",
        "            # Coronal slice (XZ plane)\n",
        "            axes[1].imshow(image_array[:, y_voxel, :], cmap='gray', vmin=-1000, vmax=400)\n",
        "            axes[1].plot(x_voxel, z_voxel, 'r+', markersize=15, markeredgewidth=2)\n",
        "            axes[1].set_title(f'Coronal Slice (Y={y_voxel})')\n",
        "            axes[1].set_xlabel('X')\n",
        "            axes[1].set_ylabel('Z')\n",
        "\n",
        "            # Sagittal slice (YZ plane)\n",
        "            axes[2].imshow(image_array[:, :, x_voxel], cmap='gray', vmin=-1000, vmax=400)\n",
        "            axes[2].plot(y_voxel, z_voxel, 'r+', markersize=15, markeredgewidth=2)\n",
        "            axes[2].set_title(f'Sagittal Slice (X={x_voxel})')\n",
        "            axes[2].set_xlabel('Y')\n",
        "            axes[2].set_ylabel('Z')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Check if the patch looks like a nodule\n",
        "            if patch.std() < 50:\n",
        "                print(\"\\n‚ùå WARNING: Patch has very low variation - might be empty space!\")\n",
        "            elif patch.mean() < -500:\n",
        "                print(\"\\n‚ùå WARNING: Patch is mostly air (mean HU < -500)\")\n",
        "            else:\n",
        "                print(\"\\n‚úì Patch looks reasonable\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå Coordinates OUT OF BOUNDS!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Transform failed: {e}\")"
      ],
      "metadata": {
        "id": "p4il5I8kQ1I2"
      },
      "id": "p4il5I8kQ1I2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CHECK ANNOTATION VALIDITY diagnostic\n",
        "\"\"\"\n",
        "# Let's see if ANY nodules are being found correctly\n",
        "print(\"üîç Checking multiple cancer cases...\\n\")\n",
        "\n",
        "cancer_cases = annotations_df[annotations_df['Malignant_lbl'] == 1].head(5)\n",
        "\n",
        "for idx, row in cancer_cases.iterrows():\n",
        "    patient_id = str(row['patient-id']).strip()\n",
        "    diameter = row['w']\n",
        "\n",
        "    print(f\"Patient {patient_id}: Diameter {diameter:.2f}mm\")\n",
        "\n",
        "    # Find file\n",
        "    patient_file = None\n",
        "    for f in patient_files:\n",
        "        if f.stem.replace('.nii', '') == patient_id:\n",
        "            patient_file = f\n",
        "            break\n",
        "\n",
        "    if patient_file:\n",
        "        image_sitk = sitk.ReadImage(str(patient_file))\n",
        "        image_array = sitk.GetArrayFromImage(image_sitk)\n",
        "\n",
        "        try:\n",
        "            point_world = (float(row['coordX']), float(row['coordY']), float(row['coordZ']))\n",
        "            idx_voxel = image_sitk.TransformPhysicalPointToIndex(point_world)\n",
        "            x_v, y_v, z_v = idx_voxel\n",
        "\n",
        "            # Check HU value at exact coordinate\n",
        "            hu_at_center = image_array[z_v, y_v, x_v]\n",
        "\n",
        "            # Check 5x5x5 region around it\n",
        "            patch_3d = image_array[\n",
        "                max(0, z_v-2):z_v+3,\n",
        "                max(0, y_v-2):y_v+3,\n",
        "                max(0, x_v-2):x_v+3\n",
        "            ]\n",
        "\n",
        "            print(f\"  HU at center: {hu_at_center:.0f}\")\n",
        "            print(f\"  5x5x5 region mean: {patch_3d.mean():.0f}\")\n",
        "\n",
        "            # Nodules should be -400 to +100 HU range\n",
        "            if hu_at_center < -500:\n",
        "                print(f\"  ‚ùå CENTER IS AIR!\")\n",
        "            elif -500 <= hu_at_center <= 100:\n",
        "                print(f\"  ‚úì Looks like tissue/nodule\")\n",
        "            else:\n",
        "                print(f\"  ‚ö†Ô∏è Unusual HU value\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error: {e}\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "oarfZF5ZRg8j"
      },
      "id": "oarfZF5ZRg8j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(\"üß™ STARTING FINE-TUNING EXPERIMENT (Weight: 3.75)...\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. LOAD MODEL (Reset to Original)\n",
        "# ==============================================================================\n",
        "target_path = Path(\"/content/drive/MyDrive/duke_lung_data/outputs/HGNN_LungCancer_MultiClass_v1.0/models/best_model.pth\")\n",
        "if not target_path.exists():\n",
        "    found = list(Path(\"/content/drive/MyDrive/duke_lung_data\").rglob(\"best_model.pth\"))\n",
        "    best_model_path = max(found, key=os.path.getmtime) if found else Path(\"/content/checkpoints/best_model.pth\")\n",
        "else:\n",
        "    best_model_path = target_path\n",
        "\n",
        "print(f\"üîÑ Resetting weights from: {best_model_path.name}\")\n",
        "checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
        "\n",
        "try:\n",
        "    if 'model_state_dict' in checkpoint: model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else: model.load_state_dict(checkpoint)\n",
        "    model.to(device)\n",
        "except:\n",
        "    try: in_feats = dataset[0].x.shape[1]\n",
        "    except: in_feats = 128\n",
        "    model = HypergraphNeuralNetwork(num_features=in_feats, hidden_dim=256, num_classes=2, dropout=0.5).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. THE CONFIGURATION (Weight 3.75)\n",
        "# ==============================================================================\n",
        "# A. Use STANDARD Loader (No Oversampling) to protect Accuracy\n",
        "active_loader = train_loader\n",
        "print(f\"üìâ Using STANDARD Imbalanced Loader ({len(train_loader.dataset)} samples)\")\n",
        "\n",
        "# B. Use MANUAL Weight (3.75) - Testing the Edge\n",
        "weights = torch.tensor([1.0, 3.75]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. TRAINING LOOP\n",
        "# ==============================================================================\n",
        "print(f\"\\n‚ñ∂Ô∏è Fine-tuning for 20 Epochs with Weight 3.75...\")\n",
        "best_f1 = 0.0\n",
        "final_model_state = None\n",
        "\n",
        "for epoch in range(1, 21):\n",
        "    model.train()\n",
        "    for batch in active_loader: # Standard Loader\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "        except: out = model(batch.x, batch.edge_index)\n",
        "        if isinstance(out, tuple): out = out[0]\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch.to(device)\n",
        "            try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            except: out = model(batch.x, batch.edge_index)\n",
        "            if isinstance(out, tuple): out = out[0]\n",
        "            preds = out.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
        "    cancer_f1 = report['1']['f1-score']\n",
        "    cancer_recall = report['1']['recall']\n",
        "    current_acc = report['accuracy']\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | Acc: {current_acc:.2f} | Recall: {cancer_recall:.2f} | F1: {cancer_f1:.2f}\")\n",
        "\n",
        "    # Save best F1\n",
        "    if cancer_f1 >= best_f1:\n",
        "        best_f1 = cancer_f1\n",
        "        final_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# 4. Save & Plot\n",
        "if final_model_state:\n",
        "    model.load_state_dict(final_model_state)\n",
        "    save_path = \"/content/drive/MyDrive/duke_lung_data/best_model_w3_75.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"\\n‚úÖ Model (W=3.75) saved to: {save_path}\")\n",
        "\n",
        "    print(\"\\nüìä GENERATING CONFUSION MATRIX (W=3.75)...\")\n",
        "    model.eval()\n",
        "    final_preds = []\n",
        "    final_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch.to(device)\n",
        "            try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            except: out = model(batch.x, batch.edge_index)\n",
        "            if isinstance(out, tuple): out = out[0]\n",
        "            final_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "            final_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(final_labels, final_preds)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Benign', 'Cancer'],\n",
        "                yticklabels=['Benign', 'Cancer'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Weight 3.75 Model')\n",
        "    plt.show()\n",
        "\n",
        "    print(classification_report(final_labels, final_preds, target_names=['Benign', 'Cancer']))"
      ],
      "metadata": {
        "id": "jeLTkhDW-Pnm"
      },
      "id": "jeLTkhDW-Pnm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(\"üß™ STARTING STRATEGY B TEST (Weight 3.75, NO Oversampling)...\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. LOAD MODEL (Reset to Original)\n",
        "# ==============================================================================\n",
        "target_path = Path(\"/content/drive/MyDrive/duke_lung_data/outputs/HGNN_LungCancer_MultiClass_v1.0/models/best_model.pth\")\n",
        "if not target_path.exists():\n",
        "    found = list(Path(\"/content/drive/MyDrive/duke_lung_data\").rglob(\"best_model.pth\"))\n",
        "    best_model_path = max(found, key=os.path.getmtime) if found else Path(\"/content/checkpoints/best_model.pth\")\n",
        "else:\n",
        "    best_model_path = target_path\n",
        "\n",
        "print(f\"üîÑ Resetting weights from: {best_model_path.name}\")\n",
        "checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
        "\n",
        "try:\n",
        "    if 'model_state_dict' in checkpoint: model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else: model.load_state_dict(checkpoint)\n",
        "    model.to(device)\n",
        "except:\n",
        "    try: in_feats = dataset[0].x.shape[1]\n",
        "    except: in_feats = 128\n",
        "    model = HypergraphNeuralNetwork(num_features=in_feats, hidden_dim=256, num_classes=2, dropout=0.5).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. THE CRITICAL SETUP (Strategy B)\n",
        "# ==============================================================================\n",
        "# A. Use STANDARD Loader (Real, Imbalanced Data)\n",
        "# This prevents the \"Double Penalty\" that caused 11% accuracy.\n",
        "active_loader = train_loader\n",
        "print(f\"üìâ Using STANDARD Imbalanced Loader ({len(train_loader.dataset)} samples)\")\n",
        "\n",
        "# B. Use CALCULATED Weight (6)\n",
        "# This forces the model to learn from the imbalance mathematically.\n",
        "weights = torch.tensor([1.0, 6]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. TRAINING LOOP\n",
        "# ==============================================================================\n",
        "print(f\"\\n‚ñ∂Ô∏è Fine-tuning for 20 Epochs with Weight 6...\")\n",
        "best_f1 = 0.0\n",
        "final_model_state = None\n",
        "\n",
        "for epoch in range(1, 21):\n",
        "    model.train()\n",
        "    for batch in active_loader: # Standard Loader\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "        except: out = model(batch.x, batch.edge_index)\n",
        "        if isinstance(out, tuple): out = out[0]\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch.to(device)\n",
        "            try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            except: out = model(batch.x, batch.edge_index)\n",
        "            if isinstance(out, tuple): out = out[0]\n",
        "            preds = out.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
        "    cancer_f1 = report['1']['f1-score']\n",
        "    cancer_recall = report['1']['recall']\n",
        "    current_acc = report['accuracy']\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | Acc: {current_acc:.2f} | Recall: {cancer_recall:.2f} | F1: {cancer_f1:.2f}\")\n",
        "\n",
        "    # Save best F1 (Balanced Metric)\n",
        "    if cancer_f1 >= best_f1:\n",
        "        best_f1 = cancer_f1\n",
        "        final_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# 4. Save & Plot\n",
        "if final_model_state:\n",
        "    model.load_state_dict(final_model_state)\n",
        "    save_path = \"/content/drive/MyDrive/duke_lung_data/best_model_strategy_b_test.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"\\n‚úÖ Strategy B Test model saved to: {save_path}\")\n",
        "\n",
        "    print(\"\\nüìä GENERATING CONFUSION MATRIX...\")\n",
        "    model.eval()\n",
        "    final_preds = []\n",
        "    final_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch.to(device)\n",
        "            try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            except: out = model(batch.x, batch.edge_index)\n",
        "            if isinstance(out, tuple): out = out[0]\n",
        "            final_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "            final_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(final_labels, final_preds)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', cbar=False,\n",
        "                xticklabels=['Benign', 'Cancer'],\n",
        "                yticklabels=['Benign', 'Cancer'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Strategy B: Weight 3.75 + Standard Data')\n",
        "    plt.show()\n",
        "\n",
        "    print(classification_report(final_labels, final_preds, target_names=['Benign', 'Cancer']))"
      ],
      "metadata": {
        "id": "DDyHUe9g_hrM"
      },
      "id": "DDyHUe9g_hrM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(\"üß™ STARTING ADJUSTED RUN (Weight 3.75, LR 5e-5)...\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. LOAD MODEL\n",
        "# ==============================================================================\n",
        "target_path = Path(\"/content/drive/MyDrive/duke_lung_data/outputs/HGNN_LungCancer_MultiClass_v1.0/models/best_model.pth\")\n",
        "if not target_path.exists():\n",
        "    found = list(Path(\"/content/drive/MyDrive/duke_lung_data\").rglob(\"best_model.pth\"))\n",
        "    best_model_path = max(found, key=os.path.getmtime) if found else Path(\"/content/checkpoints/best_model.pth\")\n",
        "else:\n",
        "    best_model_path = target_path\n",
        "\n",
        "print(f\"üîÑ Resetting weights from: {best_model_path.name}\")\n",
        "checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
        "\n",
        "try:\n",
        "    if 'model_state_dict' in checkpoint: model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else: model.load_state_dict(checkpoint)\n",
        "    model.to(device)\n",
        "except:\n",
        "    try: in_feats = dataset[0].x.shape[1]\n",
        "    except: in_feats = 128\n",
        "    model = HypergraphNeuralNetwork(num_features=in_feats, hidden_dim=256, num_classes=2, dropout=0.5).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CONFIGURATION (The Fix)\n",
        "# ==============================================================================\n",
        "active_loader = train_loader\n",
        "\n",
        "# Weight 3.75\n",
        "weights = torch.tensor([1.0, 3.75]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "# OPTIMIZED LEARNING RATE: 5e-5 (0.00005)\n",
        "# This is the \"Sweet Spot\"\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-2)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. TRAINING LOOP\n",
        "# ==============================================================================\n",
        "print(f\"\\n‚ñ∂Ô∏è Fine-tuning for 20 Epochs...\")\n",
        "best_f1 = 0.0\n",
        "final_model_state = None\n",
        "\n",
        "for epoch in range(1, 21):\n",
        "    model.train()\n",
        "    for batch in active_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "        except: out = model(batch.x, batch.edge_index)\n",
        "        if isinstance(out, tuple): out = out[0]\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Keep safety clip\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch.to(device)\n",
        "            try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            except: out = model(batch.x, batch.edge_index)\n",
        "            if isinstance(out, tuple): out = out[0]\n",
        "            preds = out.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
        "    cancer_f1 = report['1']['f1-score']\n",
        "    cancer_recall = report['1']['recall']\n",
        "    current_acc = report['accuracy']\n",
        "\n",
        "    scheduler.step(cancer_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | Acc: {current_acc:.2f} | Recall: {cancer_recall:.2f} | F1: {cancer_f1:.2f}\")\n",
        "\n",
        "    if cancer_f1 >= best_f1:\n",
        "        best_f1 = cancer_f1\n",
        "        final_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# 4. Save\n",
        "if final_model_state:\n",
        "    model.load_state_dict(final_model_state)\n",
        "    save_path = \"/content/drive/MyDrive/duke_lung_data/best_model_optimized.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"\\n‚úÖ Optimized model saved to: {save_path}\")\n",
        "\n",
        "    print(\"\\nüìä GENERATING CONFUSION MATRIX...\")\n",
        "    model.eval()\n",
        "    final_preds = []\n",
        "    final_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch.to(device)\n",
        "            try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            except: out = model(batch.x, batch.edge_index)\n",
        "            if isinstance(out, tuple): out = out[0]\n",
        "            final_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "            final_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(final_labels, final_preds)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', cbar=False,\n",
        "                xticklabels=['Benign', 'Cancer'],\n",
        "                yticklabels=['Benign', 'Cancer'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Optimized LR (5e-5) + Weight 3.75')\n",
        "    plt.show()\n",
        "\n",
        "    print(classification_report(final_labels, final_preds, target_names=['Benign', 'Cancer']))"
      ],
      "metadata": {
        "id": "Ksalb6dFDDOu"
      },
      "id": "Ksalb6dFDDOu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Set your root path\n",
        "root_path = Path(\"/content/drive/MyDrive/duke_lung_data\")\n",
        "\n",
        "# 1. Clean up Output Folders (Optional, but good for hygiene)\n",
        "old_folder = root_path / \"outputs\" / \"HGNN_LungCancer_v1.0\"\n",
        "if old_folder.exists():\n",
        "    shutil.rmtree(old_folder)\n",
        "    print(f\"üóëÔ∏è Deleted old output folder: {old_folder.name}\")\n",
        "\n",
        "# 2. THE IMPORTANT PART: Delete the Dataset Cache\n",
        "processed_folder = root_path / \"processed\"\n",
        "found_cache = False\n",
        "\n",
        "if processed_folder.exists():\n",
        "    for file in processed_folder.glob(\"*.pth\"):\n",
        "        os.remove(file)\n",
        "        print(f\"üî• DELETED CACHE FILE: {file.name} (Now the code will see 480 patients!)\")\n",
        "        found_cache = True\n",
        "    for file in processed_folder.glob(\"*.pt\"):\n",
        "        os.remove(file)\n",
        "        print(f\"üî• DELETED CACHE FILE: {file.name}\")\n",
        "        found_cache = True\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Could not find 'processed' folder. Check if it's inside 'data' or another subfolder.\")\n",
        "\n",
        "if not found_cache:\n",
        "    print(\"‚ÑπÔ∏è No cache file found. You might be ready to run dataset.py immediately.\")"
      ],
      "metadata": {
        "id": "HKfVmsztJrxy"
      },
      "id": "HKfVmsztJrxy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"üß™ STARTING ADJUSTED RUN (Weight 4.1)...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. INITIALIZE FRESH MODEL\n",
        "# ==============================================================================\n",
        "try:\n",
        "    sample = next(iter(train_loader))\n",
        "    in_feats = sample.x.shape[1]\n",
        "except:\n",
        "    in_feats = 128\n",
        "\n",
        "model = HypergraphNeuralNetwork(\n",
        "    in_channels=in_feats,\n",
        "    hidden_channels=256,\n",
        "    num_classes=2,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CONFIGURATION (Weight 4.1)\n",
        "# ==============================================================================\n",
        "# Specific request: Weight 4.1\n",
        "weights = torch.tensor([1.0, 4.1]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "# Standard Safe Learning Rate\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. TRAINING LOOP\n",
        "# ==============================================================================\n",
        "print(f\"\\n‚ñ∂Ô∏è Training for 20 Epochs...\")\n",
        "best_f1 = 0.0\n",
        "final_model_state = None\n",
        "\n",
        "for epoch in range(1, 21):\n",
        "    model.train()\n",
        "    # Using the Balanced Loader\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "        except: out = model(batch.x, batch.edge_index)\n",
        "\n",
        "        # ‚úÖ TUPLE FIX\n",
        "        if isinstance(out, tuple): out = out[0]\n",
        "\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch.to(device)\n",
        "            try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            except: out = model(batch.x, batch.edge_index)\n",
        "\n",
        "            # ‚úÖ TUPLE FIX\n",
        "            if isinstance(out, tuple): out = out[0]\n",
        "\n",
        "            preds = out.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    # Metrics\n",
        "    report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
        "    cancer_f1 = report['1']['f1-score']\n",
        "    cancer_recall = report['1']['recall']\n",
        "    current_acc = report['accuracy']\n",
        "\n",
        "    scheduler.step(cancer_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | Acc: {current_acc:.2f} | Recall: {cancer_recall:.2f} | F1: {cancer_f1:.2f}\")\n",
        "\n",
        "    if cancer_f1 >= best_f1:\n",
        "        best_f1 = cancer_f1\n",
        "        final_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. SAVE & REPORT\n",
        "# ==============================================================================\n",
        "if final_model_state:\n",
        "    model.load_state_dict(final_model_state)\n",
        "    save_path = \"/content/drive/MyDrive/duke_lung_data/best_model_w4_1.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"\\n‚úÖ Model (Weight 4.1) saved to: {save_path}\")\n",
        "\n",
        "    print(\"\\nüìä FINAL MATRIX (Weight 4.1):\")\n",
        "    model.eval()\n",
        "    final_preds = []\n",
        "    final_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch.to(device)\n",
        "            try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            except: out = model(batch.x, batch.edge_index)\n",
        "            if isinstance(out, tuple): out = out[0] # ‚úÖ TUPLE FIX\n",
        "            final_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "            final_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(final_labels, final_preds)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', cbar=False,\n",
        "                xticklabels=['Benign', 'Cancer'],\n",
        "                yticklabels=['Benign', 'Cancer'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Weight 4.1 Results')\n",
        "    plt.show()\n",
        "\n",
        "    print(classification_report(final_labels, final_preds, target_names=['Benign', 'Cancer']))"
      ],
      "metadata": {
        "id": "SKGP35liIgtq"
      },
      "id": "SKGP35liIgtq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, Subset\n",
        "from sklearn.metrics import recall_score, f1_score, accuracy_score\n",
        "from collections import Counter\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure we have the model class available\n",
        "# (If it's imported from src.models, we use that. If not, we define a compatible placeholder)\n",
        "try:\n",
        "    from src.models import HypergraphNeuralNetwork\n",
        "except ImportError:\n",
        "    # Fallback definition if import fails\n",
        "    class HypergraphNeuralNetwork(nn.Module):\n",
        "        def __init__(self, in_channels, hidden_channels, num_classes, num_layers=3, dropout=0.5):\n",
        "            super(HypergraphNeuralNetwork, self).__init__()\n",
        "            self.conv1 = nn.Linear(in_channels, hidden_channels)\n",
        "            self.conv2 = nn.Linear(hidden_channels, hidden_channels)\n",
        "            self.classifier = nn.Linear(hidden_channels, num_classes)\n",
        "            self.dropout = dropout\n",
        "\n",
        "        def forward(self, x, edge_index, batch=None):\n",
        "            x = F.relu(self.conv1(x))\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = F.relu(self.conv2(x))\n",
        "            if batch is not None:\n",
        "                from torch_geometric.nn import global_mean_pool\n",
        "                x = global_mean_pool(x, batch)\n",
        "            else:\n",
        "                x = x.mean(dim=0, keepdim=True)\n",
        "            return self.classifier(x)\n",
        "\n",
        "print(\"‚ò¢Ô∏è STARTING CLINICAL-GRADE RUN (Focal-Tversky + OneCycle)...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. DEFINE FOCAL-TVERSKY LOSS\n",
        "# ==============================================================================\n",
        "class FocalTverskyLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.7, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        probs = F.softmax(inputs, dim=1)\n",
        "        targets_one_hot = F.one_hot(targets, num_classes=2).float()\n",
        "        tp = (probs * targets_one_hot).sum(dim=0)\n",
        "        fp = (probs * (1 - targets_one_hot)).sum(dim=0)\n",
        "        fn = ((1 - probs) * targets_one_hot).sum(dim=0)\n",
        "        tversky = tp / (tp + self.alpha * fn + (1 - self.alpha) * fp + 1e-7)\n",
        "        focal_tversky = (1 - tversky) ** self.gamma\n",
        "        weights = torch.tensor([1.0, 5.0]).to(inputs.device)\n",
        "        loss = (focal_tversky * weights).mean()\n",
        "        return loss\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. RESTORE DATA & SPLITS\n",
        "# ==============================================================================\n",
        "if 'dataset' not in locals():\n",
        "    print(\"üîÑ Reloading dataset...\")\n",
        "    # Assuming the Dataset class is already defined in your session\n",
        "    dataset = LungCancerDataset(root_dir='/content/drive/MyDrive/duke_lung_data')\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_indices = list(range(train_size))\n",
        "val_indices = list(range(train_size, len(dataset)))\n",
        "\n",
        "train_subset = Subset(dataset, train_indices)\n",
        "val_subset = Subset(dataset, val_indices)\n",
        "print(f\"‚úÖ Data Ready: {len(train_subset)} Train / {len(val_subset)} Val\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. AGGRESSIVE SAMPLER SETUP\n",
        "# ==============================================================================\n",
        "train_labels = [train_subset[i].y.item() for i in range(len(train_subset))]\n",
        "counts = Counter(train_labels)\n",
        "n_benign, n_cancer = counts[0], counts[1]\n",
        "\n",
        "# Make cancer appear 10x more often\n",
        "cancer_weight = 10.0 / (n_cancer + 1e-6)\n",
        "benign_weight = 1.0 / (n_benign + 1e-6)\n",
        "sample_weights = torch.tensor([benign_weight if l == 0 else cancer_weight for l in train_labels])\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights.type(torch.DoubleTensor),\n",
        "    num_samples=len(sample_weights) * 2,\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=16, sampler=sampler, collate_fn=collate_hypergraph_batch)\n",
        "val_loader = DataLoader(val_subset, batch_size=16, shuffle=False, collate_fn=collate_hypergraph_batch)\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. MODEL & OPTIMIZER SETUP (FIXED)\n",
        "# ==============================================================================\n",
        "try:\n",
        "    sample = next(iter(train_loader))\n",
        "    in_feats = sample.x.shape[1]\n",
        "except:\n",
        "    in_feats = 128\n",
        "\n",
        "# ‚úÖ FIX: Explicitly naming arguments to avoid TypeError\n",
        "model = HypergraphNeuralNetwork(\n",
        "    in_channels=in_feats,\n",
        "    hidden_channels=256,\n",
        "    num_classes=2,\n",
        "    num_layers=3,      # Standard depth\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "criterion = FocalTverskyLoss(alpha=0.7, gamma=2.0)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)\n",
        "\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=5e-4,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.3,\n",
        "    anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. TRAINING LOOP\n",
        "# ==============================================================================\n",
        "def find_optimal_threshold(model, loader):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            except: out = model(batch.x, batch.edge_index)\n",
        "            if isinstance(out, tuple): out = out[0]\n",
        "\n",
        "            probs = F.softmax(out, dim=1)[:, 1]\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_thresh = 0.5\n",
        "\n",
        "    for t in np.arange(0.1, 0.9, 0.05):\n",
        "        preds = (all_probs >= t).astype(int)\n",
        "        recall = recall_score(all_labels, preds, zero_division=0)\n",
        "        f1 = f1_score(all_labels, preds, zero_division=0)\n",
        "\n",
        "        # Prioritize Recall > 0.60\n",
        "        if recall >= 0.60 and f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_thresh = t\n",
        "\n",
        "    return best_thresh, best_f1\n",
        "\n",
        "print(f\"\\nüöÄ Training for 50 Epochs...\")\n",
        "best_val_recall = 0.0\n",
        "final_state = None\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "        except: out = model(batch.x, batch.edge_index)\n",
        "        if isinstance(out, tuple): out = out[0]\n",
        "\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        thresh, f1 = find_optimal_threshold(model, val_loader)\n",
        "\n",
        "        model.eval()\n",
        "        probs, labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                except: out = model(batch.x, batch.edge_index)\n",
        "                if isinstance(out, tuple): out = out[0]\n",
        "                probs.extend(F.softmax(out, dim=1)[:, 1].cpu().numpy())\n",
        "                labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "        final_preds = (np.array(probs) >= thresh).astype(int)\n",
        "        recall = recall_score(labels, final_preds, zero_division=0)\n",
        "        acc = accuracy_score(labels, final_preds)\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | Loss: {train_loss/len(train_loader):.4f} | Optimal Thresh: {thresh:.2f} | Acc: {acc:.2f} | Recall: {recall:.2f} | F1: {f1:.2f}\")\n",
        "\n",
        "        if recall >= best_val_recall and acc > 0.50:\n",
        "            best_val_recall = recall\n",
        "            final_state = copy.deepcopy(model.state_dict())\n",
        "            print(f\"   üî• New Best Model Saved (Recall: {recall:.2f})\")\n",
        "\n",
        "if final_state:\n",
        "    model.load_state_dict(final_state)\n",
        "    torch.save(model.state_dict(), \"/content/drive/MyDrive/duke_lung_data/best_model_clinical.pth\")\n",
        "    print(\"\\n‚úÖ Clinical Model Saved.\")"
      ],
      "metadata": {
        "id": "LYERf-9Y09tC"
      },
      "id": "LYERf-9Y09tC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, Subset\n",
        "from sklearn.metrics import recall_score, f1_score, accuracy_score, classification_report\n",
        "from collections import Counter\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"‚öñÔ∏è STARTING BALANCED RUN (Sampler + Neutral Loss)...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. NEUTRAL FOCAL-TVERSKY LOSS (The Fix)\n",
        "# ==============================================================================\n",
        "class NeutralFocalTverskyLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, gamma=2.0): # Alpha 0.5 = Neutral Balance\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        probs = F.softmax(inputs, dim=1)\n",
        "        targets_one_hot = F.one_hot(targets, num_classes=2).float()\n",
        "\n",
        "        tp = (probs * targets_one_hot).sum(dim=0)\n",
        "        fp = (probs * (1 - targets_one_hot)).sum(dim=0)\n",
        "        fn = ((1 - probs) * targets_one_hot).sum(dim=0)\n",
        "\n",
        "        # Tversky Index\n",
        "        tversky = tp / (tp + self.alpha * fn + (1 - self.alpha) * fp + 1e-7)\n",
        "        focal_tversky = (1 - tversky) ** self.gamma\n",
        "\n",
        "        # ‚ö†Ô∏è CRITICAL FIX: NEUTRAL WEIGHTS [1.0, 1.0]\n",
        "        # The Sampler already balances the data. We don't need extra weights here.\n",
        "        weights = torch.tensor([1.0, 1.0]).to(inputs.device)\n",
        "        loss = (focal_tversky * weights).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. RESTORE DATA (Standard)\n",
        "# ==============================================================================\n",
        "if 'dataset' not in locals():\n",
        "    print(\"üîÑ Reloading dataset...\")\n",
        "    dataset = LungCancerDataset(root_dir='/content/drive/MyDrive/duke_lung_data')\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_indices = list(range(train_size))\n",
        "val_indices = list(range(train_size, len(dataset)))\n",
        "\n",
        "train_subset = Subset(dataset, train_indices)\n",
        "val_subset = Subset(dataset, val_indices)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. SAMPLER (Keep this!)\n",
        "# ==============================================================================\n",
        "train_labels = [train_subset[i].y.item() for i in range(len(train_subset))]\n",
        "counts = Counter(train_labels)\n",
        "n_benign, n_cancer = counts[0], counts[1]\n",
        "\n",
        "# Make cancer appear 10x more often in the batches\n",
        "cancer_weight = 10.0 / (n_cancer + 1e-6)\n",
        "benign_weight = 1.0 / (n_benign + 1e-6)\n",
        "sample_weights = torch.tensor([benign_weight if l == 0 else cancer_weight for l in train_labels])\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights.type(torch.DoubleTensor),\n",
        "    num_samples=len(sample_weights) * 2,\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=16, sampler=sampler, collate_fn=collate_hypergraph_batch)\n",
        "val_loader = DataLoader(val_subset, batch_size=16, shuffle=False, collate_fn=collate_hypergraph_batch)\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. TRAINING SETUP\n",
        "# ==============================================================================\n",
        "try:\n",
        "    sample = next(iter(train_loader))\n",
        "    in_feats = sample.x.shape[1]\n",
        "except:\n",
        "    in_feats = 128\n",
        "\n",
        "model = HypergraphNeuralNetwork(in_feats, 256, 2, num_layers=3, dropout=0.5).to(device)\n",
        "\n",
        "# Use the Neutral Loss\n",
        "criterion = NeutralFocalTverskyLoss(alpha=0.5, gamma=2.0)\n",
        "\n",
        "# Faster LR to escape local minima\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=5e-4,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.3\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. TRAINING LOOP\n",
        "# ==============================================================================\n",
        "def find_best_threshold(model, loader):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            except: out = model(batch.x, batch.edge_index)\n",
        "            if isinstance(out, tuple): out = out[0]\n",
        "\n",
        "            probs = F.softmax(out, dim=1)[:, 1]\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    best_thresh = 0.5\n",
        "    best_score = 0\n",
        "    # Optimize for a balance of Recall and Acc\n",
        "    for t in np.arange(0.1, 0.9, 0.05):\n",
        "        preds = (np.array(all_probs) >= t).astype(int)\n",
        "        recall = recall_score(all_labels, preds, zero_division=0)\n",
        "        acc = accuracy_score(all_labels, preds)\n",
        "        # Score = Average of Recall and Accuracy\n",
        "        score = (recall + acc) / 2\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_thresh = t\n",
        "\n",
        "    return best_thresh, best_score\n",
        "\n",
        "print(\"üöÄ Training for 50 Epochs (Neutral Weights)...\")\n",
        "best_metric = 0.0\n",
        "final_state = None\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "        except: out = model(batch.x, batch.edge_index)\n",
        "        if isinstance(out, tuple): out = out[0]\n",
        "\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        thresh, score = find_best_threshold(model, val_loader)\n",
        "\n",
        "        # Calculate final stats at this threshold\n",
        "        model.eval()\n",
        "        probs, labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                try: out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                except: out = model(batch.x, batch.edge_index)\n",
        "                if isinstance(out, tuple): out = out[0]\n",
        "                probs.extend(F.softmax(out, dim=1)[:, 1].cpu().numpy())\n",
        "                labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "        final_preds = (np.array(probs) >= thresh).astype(int)\n",
        "        recall = recall_score(labels, final_preds, zero_division=0)\n",
        "        acc = accuracy_score(labels, final_preds)\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | Best Thresh: {thresh:.2f} | Acc: {acc:.2f} | Recall: {recall:.2f}\")\n",
        "\n",
        "        # We want balanced performance\n",
        "        if score > best_metric:\n",
        "            best_metric = score\n",
        "            final_state = copy.deepcopy(model.state_dict())\n",
        "            print(f\"   üî• New Best Model (Balanced Score: {score:.2f})\")\n",
        "\n",
        "if final_state:\n",
        "    model.load_state_dict(final_state)\n",
        "    torch.save(model.state_dict(), \"/content/drive/MyDrive/duke_lung_data/best_model_balanced.pth\")\n",
        "    print(\"\\n‚úÖ Balanced Model Saved.\")"
      ],
      "metadata": {
        "id": "RCRY1P3Lx7D0"
      },
      "id": "RCRY1P3Lx7D0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "from torch_geometric.data import Data, InMemoryDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. DEFINE HYPERGRAPH CONSTRUCTOR (Dependency)\n",
        "# ==============================================================================\n",
        "class HypergraphConstructor:\n",
        "    def __init__(self, k_neighbors=5, m_clusters=10):\n",
        "        self.k = k_neighbors\n",
        "        self.m = m_clusters\n",
        "\n",
        "    def construct_graph(self, features):\n",
        "        # Create a simple star-like hypergraph structure for the feature vector\n",
        "        # Node features: The radiomics features\n",
        "        # Edges: Connections between related features\n",
        "\n",
        "        num_nodes = features.shape[0]\n",
        "\n",
        "        # 1. Edge Index (All-to-All or KNN)\n",
        "        # For a single patch, we treat features as nodes\n",
        "        # Here we create a fully connected graph for simplicity in this context\n",
        "        rows = []\n",
        "        cols = []\n",
        "        for i in range(num_nodes):\n",
        "            for j in range(num_nodes):\n",
        "                rows.append(i)\n",
        "                cols.append(j)\n",
        "\n",
        "        edge_index = torch.tensor([rows, cols], dtype=torch.long)\n",
        "\n",
        "        # 2. Node Features\n",
        "        x = torch.tensor(features, dtype=torch.float).unsqueeze(1) # [18, 1]\n",
        "\n",
        "        # 3. Dynamic Edges (Hyperedges)\n",
        "        # We can simulate hyperedges by connecting features to a central \"patch node\"\n",
        "        # But for HGNN standard input, we usually need H (Incidence Matrix) or edge_index\n",
        "\n",
        "        return x, edge_index\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. DEFINE DATASET CLASS (The Missing Piece)\n",
        "# ==============================================================================\n",
        "class LungCancerDataset(InMemoryDataset):\n",
        "    def __init__(self, root_dir, transform=None, pre_transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        # Check if AdvancedPreprocessor exists in memory, otherwise error\n",
        "        if 'AdvancedPreprocessor' not in globals():\n",
        "             raise NameError(\"‚ùå AdvancedPreprocessor not found! Please run the Preprocessor cell first.\")\n",
        "\n",
        "        self.preprocessor = AdvancedPreprocessor()\n",
        "        self.graph_constructor = HypergraphConstructor()\n",
        "\n",
        "        super(LungCancerDataset, self).__init__(root_dir, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return []\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['duke_lung_dataset.pth']\n",
        "\n",
        "    def download(self):\n",
        "        pass # Data is already local\n",
        "\n",
        "    def process(self):\n",
        "        print(f\"üè≠ PROCESSING DATASET from {self.root_dir}...\")\n",
        "\n",
        "        # 1. Load Labels\n",
        "        csv_path = self.root_dir / \"Annotation_Boxes.csv\"\n",
        "        if not csv_path.exists():\n",
        "            raise FileNotFoundError(f\"CSV not found at {csv_path}\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        data_list = []\n",
        "\n",
        "        # 2. Iterate and Process\n",
        "        print(f\"   - Found {len(df)} patients in CSV\")\n",
        "\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting Patches\"):\n",
        "            try:\n",
        "                pid = str(row['Patient_ID'])\n",
        "                # Handle path variations (00001 vs 1)\n",
        "                img_path = list(self.root_dir.glob(f\"**/*{pid}*/*.nii.gz\"))\n",
        "                if not img_path:\n",
        "                    continue\n",
        "                img_path = img_path[0]\n",
        "\n",
        "                # A. Load Image\n",
        "                img_array, spacing, origin, _, _ = self.preprocessor.load_nifti(img_path)\n",
        "                if img_array is None: continue\n",
        "\n",
        "                # B. Normalize\n",
        "                img_norm = self.preprocessor.normalize_hu(img_array)\n",
        "\n",
        "                # C. Extract Patch (USING NEW ADAPTIVE LOGIC)\n",
        "                # Coordinates from CSV are usually (x, y, z) or (z, y, x).\n",
        "                # Duke dataset is typically (x, y, z) in CSV, need to flip for numpy (z, y, x)\n",
        "                center = (row['Start_z'] + row['End_z']) // 2, \\\n",
        "                         (row['Start_y'] + row['End_y']) // 2, \\\n",
        "                         (row['Start_x'] + row['End_x']) // 2\n",
        "\n",
        "                # Default to 10mm if 'Box_Diameter' missing, or calculate from box\n",
        "                diameter = 10.0\n",
        "                if 'End_x' in row:\n",
        "                    diameter = (row['End_x'] - row['Start_x']) * spacing[0]\n",
        "\n",
        "                # THIS IS THE KEY FIX: Using the Adaptive Patch Sizing\n",
        "                patch = self.preprocessor.extract_nodule_patch(\n",
        "                    img_norm,\n",
        "                    center,\n",
        "                    diameter_mm=diameter,\n",
        "                    spacing=spacing\n",
        "                )\n",
        "\n",
        "                # D. Extract Features\n",
        "                # Create a dummy mask for radiomics (all ones since patch is cropped)\n",
        "                mask = np.ones_like(patch)\n",
        "                features = self.preprocessor.extract_radiomics_features(patch, mask)\n",
        "\n",
        "                # E. Construct Graph\n",
        "                x, edge_index = self.graph_constructor.construct_graph(features)\n",
        "\n",
        "                # F. Label\n",
        "                y = torch.tensor([int(row['Label'])], dtype=torch.long)\n",
        "\n",
        "                # G. Save Data Object\n",
        "                data = Data(x=x, edge_index=edge_index, y=y)\n",
        "                data_list.append(data)\n",
        "\n",
        "            except Exception as e:\n",
        "                # print(f\"Skipping {pid}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # 3. Save to Disk\n",
        "        print(f\"   - Successfully processed {len(data_list)} samples.\")\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. TRIGGER BUILD\n",
        "# ==============================================================================\n",
        "print(\"\\nüöÄ Starting Dataset Generation...\")\n",
        "dataset = LungCancerDataset(root_dir='/content/drive/MyDrive/duke_lung_data')\n",
        "print(f\"‚úÖ DONE! Dataset loaded with {len(dataset)} samples.\")"
      ],
      "metadata": {
        "id": "DdszvX9szHBR"
      },
      "id": "DdszvX9szHBR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wJCiR6Jz03eP"
      },
      "id": "wJCiR6Jz03eP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}